{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home/dhuang/thesis/WavCaps\n",
      "/home/dhuang/.conda/envs/fs/lib/python310.zip\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/lib-dynload\n",
      "\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages/sentencepiece-0.1.95-py3.10-linux-x86_64.egg\n",
      "/gpfs/home/dhuang/thesis/WavCaps/retrieval/models\n",
      "/gpfs/home/dhuang/thesis/WavCaps/retrieval/tools\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/39299838/how-do-i-import-module-in-jupyter-notebook-directory-into-notebooks-in-lower-dir\n",
    "# https://stackoverflow.com/questions/68572852/import-local-modules-in-jupyter-notebook\n",
    "import os\n",
    "import sys\n",
    "# os.path.split(os.getcwd())[0]\n",
    "sys.path.append(\"/gpfs/home/dhuang/thesis/WavCaps/retrieval/models\")\n",
    "sys.path.append(\"/gpfs/home/dhuang/thesis/WavCaps/retrieval/tools\")\n",
    "for nb_dir in sys.path:\n",
    "    print(nb_dir)\n",
    "    # sys.path.append(nb_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    GPT2Model,\n",
    "    GPT2Tokenizer,\n",
    "    RobertaModel,\n",
    "    RobertaTokenizer,\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    "    CLIPTokenizer,\n",
    "    CLIPTextModel,\n",
    ")\n",
    "\n",
    "MODELS = {\n",
    "    'openai/clip-vit-base-patch32': (CLIPTextModel, CLIPTokenizer, 512),\n",
    "    'prajjwal1/bert-tiny': (BertModel, BertTokenizer, 128),\n",
    "    'prajjwal1/bert-mini': (BertModel, BertTokenizer, 256),\n",
    "    'prajjwal1/bert-small': (BertModel, BertTokenizer, 512),\n",
    "    'prajjwal1/bert-medium': (BertModel, BertTokenizer, 512),\n",
    "    'gpt2': (GPT2Model, GPT2Tokenizer, 768),\n",
    "    'distilgpt2': (GPT2Model, GPT2Tokenizer, 768),\n",
    "    'bert-base-uncased': (BertModel, BertTokenizer, 768),\n",
    "    'bert-large-uncased': (BertModel, BertTokenizer, 1024),\n",
    "    'roberta-base': (RobertaModel, RobertaTokenizer, 768),\n",
    "    'roberta-large': (RobertaModel, RobertaTokenizer, 1024),\n",
    "    'distilbert-base-uncased': (DistilBertModel, DistilBertTokenizer, 768),\n",
    "    \"distilroberta-base\": (RobertaModel, RobertaTokenizer, 768),\n",
    "}\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    GPT2Model,\n",
    "    GPT2Tokenizer,\n",
    "    RobertaModel,\n",
    "    RobertaTokenizer,\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    "    CLIPTokenizer,\n",
    "    CLIPTextModel,\n",
    ")\n",
    "\n",
    "MODELS = {\n",
    "    'openai/clip-vit-base-patch32': (CLIPTextModel, CLIPTokenizer, 512),\n",
    "    'prajjwal1/bert-tiny': (BertModel, BertTokenizer, 128),\n",
    "    'prajjwal1/bert-mini': (BertModel, BertTokenizer, 256),\n",
    "    'prajjwal1/bert-small': (BertModel, BertTokenizer, 512),\n",
    "    'prajjwal1/bert-medium': (BertModel, BertTokenizer, 512),\n",
    "    'gpt2': (GPT2Model, GPT2Tokenizer, 768),\n",
    "    'distilgpt2': (GPT2Model, GPT2Tokenizer, 768),\n",
    "    'bert-base-uncased': (BertModel, BertTokenizer, 768),\n",
    "    'bert-large-uncased': (BertModel, BertTokenizer, 1024),\n",
    "    'roberta-base': (RobertaModel, RobertaTokenizer, 768),\n",
    "    'roberta-large': (RobertaModel, RobertaTokenizer, 1024),\n",
    "    'distilbert-base-uncased': (DistilBertModel, DistilBertTokenizer, 768),\n",
    "    \"distilroberta-base\": (RobertaModel, RobertaTokenizer, 768),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In summary, the add_pooling_layer parameter allows you to control whether you want just the sequence output or both the sequence output and a pooled output. This flexibility lets you tailor the model's outputs to better fit the specific needs of your application.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# type: 'bert-base-uncased'\n",
    "tokenizer = MODELS['bert-base-uncased'][1].from_pretrained('bert-base-uncased')\n",
    "text_encoder = MODELS['bert-base-uncased'][0].from_pretrained('bert-base-uncased', add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(text,\n",
    "                        padding='longest',\n",
    "                        truncation=True,\n",
    "                        max_length=30,\n",
    "                        return_tensors=\"pt\").to('cpu')\n",
    "text_output = text_encoder(input_ids=text_input.input_ids,\n",
    "                        attention_mask=text_input.attention_mask)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1999, 12654,  1010,  1996,  5587,  1035,  4770,  2075,  1035,\n",
      "          6741, 16381,  4473,  2017,  2000,  2491,  3251,  2017,  2215,  2074,\n",
      "          1996,  5537,  6434,  2030,  2119,  1996,  5537,  6434,  1998,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_input.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0745, -0.4085, -0.0032,  ..., -0.4064, -0.1115,  0.7753],\n",
      "         [-0.4994, -0.2811, -0.2728,  ...,  0.0369,  0.7994,  0.7260],\n",
      "         [ 0.4675, -0.1879, -0.3194,  ...,  0.2579,  0.3036, -0.2950],\n",
      "         ...,\n",
      "         [ 0.4218, -0.0810, -0.0760,  ..., -0.0894,  0.0644,  0.3963],\n",
      "         [-0.5284, -0.7002, -0.5880,  ...,  0.5098,  0.7785, -0.0368],\n",
      "         [ 0.9336,  0.1749, -0.1502,  ...,  0.2229, -0.5964, -0.2550]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2000//512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveform shape before crop:  (240000,)\n",
      "waveform shape:  (160000,)\n",
      "waveform_tensor shape:  torch.Size([1, 160000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.audio_encoder import AudioEncoder\n",
    "from models.text_encoder import TextEncoder\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from tools.losses import AudioTextContrastiveLoss, NTXent\n",
    "from tools.utils import remove_grad\n",
    "import ruamel.yaml as yaml\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n",
    "from models.feature_extractor import AudioFeature\n",
    "\n",
    "\n",
    "\n",
    "config_file_path = './settings/baseline.yaml'\n",
    "\n",
    "with open(config_file_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        \n",
    "# Load audio signal file\n",
    "wav_file_path = '../../dac/audio_samples/at2_16khz_cvt.wav'\n",
    "waveform, _ = librosa.load(wav_file_path, sr=config[\"audio_args\"][\"sr\"], mono=True)\n",
    "print('waveform shape before crop: ', waveform.shape)\n",
    "if config[\"audio_args\"][\"max_length\"] != 0:\n",
    "            # if audio length is longer than max_length, we random crop it\n",
    "            max_length = config[\"audio_args\"][\"max_length\"] * config[\"audio_args\"][\"sr\"]\n",
    "            if waveform.shape[-1] > max_length:\n",
    "                max_start = waveform.shape[-1] - max_length\n",
    "                start = random.randint(0, max_start)\n",
    "                waveform = waveform[start: start + max_length]\n",
    "                \n",
    "print('waveform shape: ', waveform.shape)\n",
    "waveform_tensor = torch.tensor(waveform[None, :])\n",
    "print('waveform_tensor shape: ', waveform_tensor.shape)\n",
    "\n",
    "# AF = AudioFeature(config[\"audio_args\"])\n",
    "# audio_encoded = AF(waveform_tensor)\n",
    "# print('audio_encoded.shape: ', audio_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dac\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "model_path = Path('/home/dhuang/.cache/descript/dac/weights_16khz_8kbps_0.0.5.pth')\n",
    "# model_path = Path('/home/dhuang/.cache/descript/dac/weights_44khz_8kbps_0.0.1.pth')\n",
    "audio_enc = dac.DAC.load(model_path)\n",
    "audio_enc.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = audio_enc.preprocess(waveform_tensor, 16000)\n",
    "# x = torch.tensor(x[:,None,:])\n",
    "x = x[:,None,:].detach().clone()\n",
    "# print(\"inputs wav hz is: \", len(inputs[0]) / self.config[\"audio_args\"][\"max_length\"])\n",
    "# print(\"x shape is: \", x.shape)\n",
    "# time.sleep(10)\n",
    "# print(f\"self.training in eval mode: {not self.training}\")\n",
    "# print(f\"self.training.audio_enc in eval mode: {not self.audio_enc.training}\")\n",
    "with torch.no_grad():\n",
    "    # print(\"N_CODEBOOKS: \", self.config[\"audio_encoder_args\"][\"N_CODEBOOKS\"])\n",
    "    z, codes, latents, _, _ = audio_enc.encode(x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 350,    0,  554,  ...,  962,  167,  563],\n",
       "         [ 501,  268,  327,  ...,  885,  804,  718],\n",
       "         [ 319,  310,  463,  ...,  450,   14,  715],\n",
       "         [  84,  422, 1016,  ...,  375,  454,  922]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(codes.shape)\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2000])\n",
      "torch.Size([3, 2000])\n"
     ]
    }
   ],
   "source": [
    "codes_flatten = torch.flatten(codes, start_dim=1)#.to(torch.float)\n",
    "print(codes_flatten.shape)\n",
    "batch_size = 3\n",
    "batch_codes_flatten = codes_flatten.repeat(batch_size, 1)\n",
    "print(batch_codes_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "\n",
    "class LongCodesEmbedder(torch.nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(LongCodesEmbedder, self).__init__()\n",
    "        # self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name)\n",
    "        self.max_length = 512\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return list(self.parameters())[0].device\n",
    "\n",
    "    def forward(self, codes):\n",
    "        # Tokenize texts without truncation\n",
    "        # tokens = self.tokenizer(texts, return_tensors='pt', truncation=False, padding=True)\n",
    "        input_ids = codes # tokens['input_ids']\n",
    "        # attention_mask = tokens['attention_mask']\n",
    "\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # Prepare list to collect chunk outputs\n",
    "        all_chunks = []\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            input_ids_batch = input_ids[batch_idx]\n",
    "            # attention_mask_batch = attention_mask[batch_idx]\n",
    "\n",
    "            input_chunks = []\n",
    "            attention_chunks = []\n",
    "\n",
    "            # Split input_ids into chunks of max_length with overlap\n",
    "            chunk_size = self.max_length #- 2  # account for [CLS] and [SEP] tokens\n",
    "            overlap_size = 50\n",
    "            \n",
    "            for i in range(0, len(input_ids_batch), chunk_size - overlap_size):\n",
    "                chunk_ids = input_ids_batch[i:i + chunk_size]\n",
    "                # chunk_mask = attention_mask_batch[i:i + chunk_size]\n",
    "                chunk_mask = torch.tensor([1] * len(chunk_ids)).to(self.device)\n",
    "\n",
    "                # Add [CLS] and [SEP] tokens\n",
    "                # chunk_ids = torch.cat([torch.tensor([self.tokenizer.cls_token_id]), chunk_ids, torch.tensor([self.tokenizer.sep_token_id])])\n",
    "                # chunk_mask = torch.cat([torch.tensor([1]), chunk_mask, torch.tensor([1])])\n",
    "\n",
    "                # Pad chunks to max_length\n",
    "                padding_length = self.max_length - chunk_ids.size(0)\n",
    "                if padding_length > 0:\n",
    "                    chunk_ids = torch.cat([chunk_ids, torch.tensor([0] * padding_length)])\n",
    "                    chunk_mask = torch.cat([chunk_mask, torch.tensor([0] * padding_length)])\n",
    "\n",
    "                input_chunks.append(chunk_ids.unsqueeze(0))\n",
    "                attention_chunks.append(chunk_mask.unsqueeze(0))\n",
    "\n",
    "            # Stack chunks into a batch\n",
    "            input_chunks = torch.cat(input_chunks, dim=0).to(self.device)\n",
    "            print(input_chunks.shape)\n",
    "            attention_chunks = torch.cat(attention_chunks, dim=0).to(self.device)\n",
    "            print(attention_chunks.shape)\n",
    "\n",
    "            # Process each chunk and collect the hidden states\n",
    "            outputs = []\n",
    "            for i in range(input_chunks.size(0)):\n",
    "                chunk_output = self.model(input_ids=input_chunks[i:i+1], attention_mask=attention_chunks[i:i+1])[0]\n",
    "                outputs.append(chunk_output)\n",
    "\n",
    "            # Concatenate all chunk outputs along the sequence dimension\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "            \n",
    "            all_chunks.append(outputs.unsqueeze(0))  # Keep batch dimension\n",
    "\n",
    "        # Stack all batch outputs\n",
    "        all_chunks = torch.cat(all_chunks, dim=0)\n",
    "\n",
    "        # Optionally: apply some combination method here (e.g., mean pooling, attention)\n",
    "        # combined_output = torch.mean(all_chunks, dim=1)  # example of mean pooling\n",
    "        # Average Pooling\n",
    "        pooled_embeddings = torch.mean(all_chunks, dim=2, keepdim=True)  # Shape: [1, 1, 768]\n",
    "        # pooled_embeddings = pooled_embeddings.repeat(1, 1, 4, 1)            # Shape: [1, 4, 768]\n",
    "\n",
    "        return pooled_embeddings\n",
    "\n",
    "# Example usage\n",
    "# model = LongCodesEmbedder()\n",
    "# texts = np.ndarray([[\"Your very long text of 2000 tokens goes here...\", \"Another long text...\"]])\n",
    "# print(texts.shape)\n",
    "# embeddings = model(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([1, 1, 1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.6748e-01,  2.9985e-01,  9.8263e-01,  2.2093e-01,  3.5365e-02,\n",
       "           -5.2790e-01,  5.2103e-01,  2.9357e-02, -2.4657e-01,  1.0981e-01,\n",
       "            4.5585e-01, -3.2240e-01,  1.2481e-01,  4.0589e-01, -2.3366e-01,\n",
       "            7.4490e-01, -3.3269e-01,  1.9363e-01, -7.9094e-01,  4.0406e-01,\n",
       "            1.0193e+00,  5.7548e-01,  2.5680e-01,  8.2678e-01,  3.3883e-01,\n",
       "            7.9272e-01,  2.9397e-01, -1.0526e+00, -6.0919e-01,  1.7706e-01,\n",
       "            9.1806e-03,  3.5964e-02,  5.5864e-01, -3.6340e-02, -1.5720e-01,\n",
       "            1.5364e-02, -3.4198e-01,  1.2270e-01,  3.6486e-01, -1.5424e-01,\n",
       "           -4.1983e-01, -6.5624e-01,  5.2895e-01,  9.2712e-02,  2.0621e-01,\n",
       "           -5.3588e-01,  1.3662e-01,  5.6997e-01, -3.7826e-01, -9.0381e-01,\n",
       "           -7.6079e-01, -2.6866e-03,  6.0744e-01,  3.9286e-02,  2.5605e-01,\n",
       "            4.3356e-01, -1.0845e-01, -7.4679e-01,  7.8749e-01,  4.0365e-01,\n",
       "           -5.4367e-01,  4.6030e-01, -9.1697e-01, -1.6975e-01, -1.8310e-01,\n",
       "            6.9115e-01, -2.3710e-01, -3.3129e-01, -5.8749e-01, -9.1809e-01,\n",
       "            6.1384e-01,  4.2441e-02,  1.6940e-01,  1.7624e-01, -3.7318e-02,\n",
       "            4.2035e-01,  2.7614e-01,  5.0256e-01,  1.9494e-01,  1.6757e-01,\n",
       "           -7.3365e-02, -2.2318e-01,  2.3674e-01,  1.3067e-01,  5.4595e-01,\n",
       "            6.7014e-01,  7.1837e-01,  1.4804e-01, -6.2187e-01,  3.0540e-01,\n",
       "            4.1540e-01,  2.4061e-02,  3.2028e-01,  1.3242e-01, -3.8711e-01,\n",
       "            1.5356e-01,  6.2270e-02, -3.7354e-01, -4.9265e-01, -6.9955e-01,\n",
       "           -6.5554e-01,  5.2998e-01,  3.6431e-01, -5.7860e-01,  5.2912e-02,\n",
       "            3.5333e-01, -1.2151e+00,  1.2015e-01,  1.3431e-01,  8.1244e-01,\n",
       "            7.8948e-02,  4.6514e-01, -1.9969e-01, -4.0975e-01,  5.6380e-01,\n",
       "            5.3754e-01,  1.8619e-01, -2.0960e-01, -1.2959e-01, -2.6660e-01,\n",
       "            4.3905e-01, -3.8797e-01,  9.6507e-02,  8.6348e-01, -4.9582e-01,\n",
       "           -3.7496e-01,  1.8585e-01, -8.8075e-01,  1.6840e-01,  8.1060e-01,\n",
       "           -8.3021e-03, -1.6591e-02, -9.3801e-02,  5.3000e-01, -8.4254e-01,\n",
       "           -6.8032e-01, -8.0157e-01,  7.5131e-02, -2.0094e-02, -3.3910e-01,\n",
       "           -3.9120e-01,  5.1864e-01,  7.4838e-01, -6.5211e-01, -2.3738e-02,\n",
       "           -1.9026e-01,  1.2123e-01,  7.1841e-02, -9.2191e-01, -3.0023e-01,\n",
       "            2.9331e-01,  5.7935e-02, -4.0991e-01, -1.2311e+00,  4.9316e-02,\n",
       "           -9.9167e-01,  1.0966e+00, -1.8443e-01,  1.2991e-01,  1.6045e-01,\n",
       "           -1.3388e-01,  2.6442e-01,  1.2728e-01, -2.6503e-01,  4.5006e-02,\n",
       "            1.7814e-03,  8.2645e-02,  5.4287e-01, -2.7340e-01,  5.4165e-02,\n",
       "            3.4291e-01,  1.2516e-01,  4.3005e-01, -1.1543e+00,  2.0813e-02,\n",
       "            7.9351e-01,  5.7563e-01, -1.6501e-01,  1.0684e+00,  1.6482e-01,\n",
       "            5.2221e-02, -3.4373e-01, -3.2105e-01, -5.6126e-01,  8.3041e-01,\n",
       "            7.5818e-01, -6.8733e-01,  8.1320e-01,  4.0105e-01, -1.2942e-01,\n",
       "           -2.2867e+00, -5.5329e-01,  7.8072e-01,  3.1454e-01, -2.1923e-01,\n",
       "            6.2783e-02, -4.6241e-02,  5.0235e-01,  7.6854e-02, -8.0706e-01,\n",
       "            1.3811e-03,  1.2315e+00, -5.2011e-02,  2.1667e-01, -2.5103e-01,\n",
       "            2.2574e-01, -1.9632e-02, -4.6881e-01, -3.7124e-01, -1.0247e-01,\n",
       "           -6.1538e-01, -2.3776e-02, -4.4327e-01,  3.2513e-01, -5.0574e-01,\n",
       "           -7.3538e-01, -7.7154e-01, -1.0609e-01,  2.6571e-01,  1.4636e-01,\n",
       "           -1.0865e+00, -6.6358e-01, -3.3593e-01,  6.2646e-01, -7.1242e-01,\n",
       "           -2.2477e-01, -7.8350e-02, -8.9974e-01,  5.0947e-01,  4.2470e-02,\n",
       "            4.3838e-01, -1.2865e+00,  1.9999e-01, -9.9367e-01, -5.8438e-01,\n",
       "           -6.6983e-02, -2.0416e-01, -4.2288e-01,  7.5387e-02, -2.7401e-01,\n",
       "           -2.7481e-01,  1.0273e+00, -4.8554e-01, -3.4496e-01, -6.3050e-02,\n",
       "           -3.2068e-01, -5.4731e-01,  1.1237e-01,  4.0138e-01, -1.9779e-01,\n",
       "           -7.6277e-01,  6.7802e-01, -3.8232e-02,  4.1895e-01,  1.9145e-01,\n",
       "           -4.4720e-02, -8.3857e-02,  2.0488e-01, -1.8653e-01, -1.8349e-01,\n",
       "            6.8083e-01,  4.8003e-02,  3.9455e-01,  2.7393e-01,  1.9571e-01,\n",
       "            2.5528e-01,  4.7911e-01, -2.3933e-01,  7.5156e-02,  1.8714e-01,\n",
       "           -6.7998e-01, -1.3068e-01, -2.7052e-01,  1.4816e+00, -5.9092e-02,\n",
       "           -4.0180e-01,  9.6672e-01,  1.0580e-01,  1.4110e-02, -4.7214e-01,\n",
       "            3.0935e-01, -3.9940e-02, -3.3176e-01,  8.2839e-02, -6.3852e-02,\n",
       "           -3.8601e-01,  3.4597e-01, -1.3523e-01,  5.0062e-01,  4.2098e-02,\n",
       "            9.4114e-03,  1.0079e+00,  2.7665e-01, -7.8206e-02,  5.1774e-01,\n",
       "           -3.2698e-01, -5.0066e-01, -2.1096e-01,  7.1824e-01,  6.5331e-01,\n",
       "           -9.0875e-01,  2.9700e-01, -6.6205e-01,  1.7124e-01, -8.3343e-02,\n",
       "            3.1503e-01, -5.3113e-02, -1.8437e-01,  1.6037e-01,  8.0236e-01,\n",
       "           -4.2892e-01,  1.9636e-01,  4.6615e-01,  8.4137e-01,  1.6380e-01,\n",
       "            4.7073e-01, -2.1673e-01,  8.4167e-01,  5.7396e-01,  1.3349e-01,\n",
       "           -9.7290e-01, -9.2880e-02, -9.7520e-01, -6.5031e-01,  1.1595e+00,\n",
       "           -4.5845e-01, -6.2317e-01,  3.2081e-01, -3.3362e-01, -3.7299e-01,\n",
       "            6.2070e-01, -1.5575e-01,  5.6227e-01,  6.6451e-01,  6.0157e-01,\n",
       "           -2.5012e-01,  5.1657e-01, -5.6333e-01, -5.3100e-01, -7.2306e-01,\n",
       "            4.4522e-01,  1.2408e-02,  5.7290e-02,  1.0112e-01, -1.2935e-01,\n",
       "           -8.5148e-01, -5.3103e-01, -5.4073e-01,  7.8414e-01, -1.9760e+00,\n",
       "            4.4677e-01,  1.9917e-02,  3.7554e-01, -6.8867e-01, -1.4850e-01,\n",
       "            1.3362e+00,  1.1535e-01, -1.5929e-01, -1.8333e-01,  1.5592e-01,\n",
       "           -1.1221e+00, -1.6284e-01, -6.8673e-01, -1.7951e-01,  3.3483e-01,\n",
       "           -1.5155e-01, -1.0808e+00,  2.9524e-01,  5.9588e-01,  1.7688e-01,\n",
       "           -7.7262e-01,  9.4901e-01, -3.5741e-01, -1.1576e-01, -1.0823e+00,\n",
       "           -8.6344e-02, -1.1002e+00, -4.6344e-01, -9.9701e-01,  1.1734e-01,\n",
       "           -1.6413e-01,  6.8001e-01,  1.1272e+00, -8.6525e-01,  2.5623e-01,\n",
       "           -3.0166e-01,  2.9550e-01, -4.4744e-01,  6.9114e-02,  2.6313e-01,\n",
       "           -2.2129e-01,  1.9073e-01,  2.7607e-01,  6.3848e-01, -8.8515e-01,\n",
       "            2.4755e-01, -4.0813e-01,  8.2455e-01,  2.4211e-01, -6.3752e-01,\n",
       "            3.5081e-01, -1.3910e-02, -6.2011e-01,  5.7281e-01,  1.3399e-01,\n",
       "            1.0613e+00,  1.7615e-01, -4.1617e-02,  7.1405e-01, -3.8937e-01,\n",
       "            3.8360e-01,  3.8787e-01, -2.5055e-01,  7.8180e-01, -4.7249e-01,\n",
       "           -3.1492e-01,  1.6935e-01,  1.5986e-02,  2.7620e-01,  4.3110e-01,\n",
       "           -4.6741e-01, -5.3432e-02, -2.5356e-01, -4.1234e-01,  3.3294e-01,\n",
       "            7.6469e-01, -1.4950e-01, -3.9854e-01, -4.8188e-01,  4.8800e-01,\n",
       "           -4.1091e-01,  2.6008e-02, -9.2908e-01,  5.8927e-02, -8.9542e-02,\n",
       "           -2.2176e-01,  3.7409e-01, -3.7306e-01, -2.3757e-01, -3.1395e-01,\n",
       "           -9.3643e-01, -5.3425e-02,  3.2544e-02,  3.5328e-01,  1.0478e+00,\n",
       "           -8.7251e-01, -2.6051e-01, -7.9096e-02,  3.8052e-01,  1.0614e-01,\n",
       "           -4.8067e-01,  7.7860e-02,  1.7836e-01,  2.5373e-01,  3.2071e-01,\n",
       "           -1.3319e-01, -2.8491e-01,  7.3915e-01,  1.0861e-01,  6.3132e-02,\n",
       "            1.4643e-01, -6.3495e-02,  4.5608e-01,  9.7107e-01, -2.2280e-01,\n",
       "            8.0812e-02,  2.2342e-01, -1.0696e+00,  4.6606e-01,  2.1309e-01,\n",
       "            1.3905e-01, -7.7154e-01,  2.9741e-01,  3.7489e-01, -1.2446e-01,\n",
       "            1.2237e-01, -2.2719e-01, -2.9242e-01, -5.4245e-01,  5.8253e-01,\n",
       "            5.3519e-01,  2.7370e-01,  4.9406e-02,  7.5545e-01,  3.8631e-01,\n",
       "            7.7970e-03,  2.0530e-01,  1.2195e-01, -2.7088e-01, -2.0549e-02,\n",
       "            1.6694e-01,  1.5439e-01,  7.9094e-02,  2.8519e-01, -2.0632e-01,\n",
       "            1.9218e-01,  1.0733e-01,  1.5081e-01, -1.1654e+00, -1.2125e-02,\n",
       "           -1.7355e-01, -7.2232e-01, -1.5637e-01, -4.1079e-01, -3.1405e-02,\n",
       "           -2.8094e-01,  2.9715e-01,  2.2881e-01, -1.0287e+00,  1.8289e-01,\n",
       "            7.3195e-01, -3.9298e-01, -1.7220e-02, -1.5119e-01, -7.2199e-01,\n",
       "            2.8908e-02, -1.8384e-01,  6.2548e-02,  1.2928e-01,  1.5164e-01,\n",
       "            2.8072e-01,  5.2081e-01, -1.7252e-01, -1.2803e-01,  1.5647e-01,\n",
       "           -8.4535e-01, -2.2349e-01,  1.1901e-01, -2.5163e-01,  2.4948e-01,\n",
       "            1.0507e+00, -1.9224e-01, -2.2801e-01, -4.3767e-01, -6.6509e-02,\n",
       "            6.5099e-01,  6.9446e-01,  2.2530e-01, -6.6825e-01, -1.0651e+00,\n",
       "           -5.0814e-01, -1.2390e-01, -2.1348e-01,  6.3966e-02,  4.3409e-01,\n",
       "           -4.4524e-01,  4.3455e-01,  5.3848e-01, -3.2718e-01,  1.8270e-01,\n",
       "            3.0964e-01, -9.2955e-01,  2.4670e-01, -1.8621e-01,  1.4565e-01,\n",
       "           -2.9409e-01, -4.1110e-01, -1.2852e-01, -4.6887e-01, -1.2679e-01,\n",
       "            4.7447e-01, -3.1746e-01,  3.2752e-01, -4.6695e-01, -9.8547e-02,\n",
       "           -6.6274e-01, -1.5081e-01, -9.6303e-01, -5.7538e-01,  7.9142e-01,\n",
       "           -8.2906e-02,  1.3542e-01, -6.9192e-01, -2.3878e-01,  9.0463e-02,\n",
       "            4.9103e-01,  7.3994e-01, -8.2832e-01, -1.6922e-01,  2.8059e-01,\n",
       "            1.0922e-01, -4.1741e-01,  5.5905e-01,  4.2004e-01,  3.0311e-01,\n",
       "           -8.2903e-01,  4.8201e-01, -4.5530e-01, -8.7350e-01, -9.7948e-01,\n",
       "           -5.3578e-01,  1.3425e-02, -1.3389e-01, -1.3869e-01, -2.9225e-01,\n",
       "           -5.0625e-01,  9.0206e-02, -3.8824e-02, -7.9942e-01,  5.2612e-01,\n",
       "           -4.8772e-01, -3.9380e-01,  2.7677e-02,  2.7216e-01,  4.8003e-01,\n",
       "           -1.7733e-01,  5.5820e-01,  3.0877e-01, -1.1822e-01, -1.2535e-01,\n",
       "            6.7360e-01, -9.5828e-01, -3.4353e-01,  1.2398e-02, -3.2259e-01,\n",
       "           -4.2702e-01, -3.4212e-01, -4.3851e-01, -4.6293e-01, -2.1909e-01,\n",
       "            5.1021e-01, -4.9575e-02,  5.3090e-01, -8.4416e-01, -5.2159e-01,\n",
       "            3.4934e-01, -3.2739e-02,  4.1350e-01,  1.3118e-01,  7.8527e-01,\n",
       "            2.8924e-01, -1.3712e-01, -1.4396e-01,  4.2886e-01,  2.0178e-02,\n",
       "           -1.8368e-01, -1.4659e-01, -1.3417e-01, -4.3061e-01, -4.2098e-01,\n",
       "           -1.9696e-01, -2.9241e-01, -3.7398e-01,  3.7339e-01,  2.0832e-01,\n",
       "           -7.9470e-01, -6.6256e-01, -3.1645e-01,  1.2812e-01, -2.9380e-01,\n",
       "            2.2146e-01, -6.0686e-01, -5.7718e-01,  1.0285e-01,  2.5334e-01,\n",
       "           -8.7171e-02,  4.7965e-01, -5.2102e-02,  6.0764e-02, -1.5409e-01,\n",
       "            4.5552e-01,  2.6575e-01, -2.1217e-01, -5.3339e-01, -3.7812e-01,\n",
       "           -5.7578e-01, -4.1821e-01,  9.7229e-01, -7.3738e-02,  2.9306e-01,\n",
       "            9.5307e-02,  3.3035e-01, -6.9917e-01, -4.3387e-01,  9.9160e-01,\n",
       "            2.7739e-01,  8.9481e-01, -5.1194e-01,  9.4462e-01, -2.4754e-01,\n",
       "            5.5465e-02, -8.3198e-01, -1.0861e-02,  4.4060e-01,  4.0847e-01,\n",
       "            3.6381e-01, -1.8898e-02, -7.2711e-01, -4.9729e-01,  6.5068e-02,\n",
       "            1.1015e-01, -1.2304e-02, -9.7183e-02,  4.6579e-01, -3.7505e-01,\n",
       "           -6.5103e-01,  2.9432e-01, -5.9947e-01,  2.1557e-01, -3.2043e-01,\n",
       "            3.1368e-01,  7.8478e-01,  5.0029e-01,  7.8774e-01,  6.1557e-01,\n",
       "            3.9644e-01,  1.0585e+00,  2.2183e-01, -3.4677e-01, -5.4712e-02,\n",
       "            5.3113e-01,  6.1413e-01, -2.2425e-01, -7.3716e-02,  7.6744e-01,\n",
       "            8.0636e-01, -5.0935e-01, -2.3305e-02, -5.4837e-01, -6.0443e-01,\n",
       "           -1.9953e-02, -2.0953e-01, -7.9424e-01, -9.9898e-01,  1.9510e-02,\n",
       "           -2.9265e-01, -1.0648e-01, -7.4666e-01, -4.7215e-01, -2.3967e-01,\n",
       "            6.9836e-01,  6.0563e-01, -7.6966e-01,  2.1893e-01,  4.8572e-01,\n",
       "           -1.2376e-01, -8.1817e-01,  5.9235e-01, -1.0318e+00, -4.2057e-01,\n",
       "            4.6980e-02,  4.7510e-01, -4.2738e-01, -1.0377e+00,  1.2622e-01,\n",
       "            1.8344e-01,  3.9470e-01,  2.3477e-01,  1.2407e+00,  7.7049e-01,\n",
       "           -7.6794e-01, -6.6953e-01,  1.4631e+00,  2.0779e-01,  1.2330e-01,\n",
       "           -2.2232e-02, -5.3804e-02, -2.1514e-01, -8.5177e-01, -1.3373e-01,\n",
       "            3.3314e-01, -1.1759e-01,  5.1520e-01, -8.0519e-01,  2.7929e-01,\n",
       "           -5.1647e-01,  9.2821e-02, -7.4826e-01]]]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WITH `combined_output = torch.mean(all_chunks, dim=1)`  # example of mean pooling\n",
    "model = LongCodesEmbedder()\n",
    "# texts = np.ndarray([[\"Your very long text of 2000 tokens goes here...\", \"Another long text...\"]])\n",
    "# print(texts.shape)\n",
    "# embeddings = model(texts)\n",
    "\n",
    "text_output = model(codes_flatten)\n",
    "print(text_output.shape)\n",
    "text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([1, 1, 2560, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3144, -0.0121,  0.0405,  ..., -0.5311,  0.5728, -0.1658],\n",
       "          [-0.5853,  0.2675, -0.1129,  ..., -0.8531,  0.3428,  0.0660],\n",
       "          [-0.6087,  0.2474, -0.0817,  ..., -0.8487,  0.2745,  0.0873],\n",
       "          ...,\n",
       "          [ 0.2932,  0.3740,  1.2786,  ..., -0.5581, -0.0460, -1.3185],\n",
       "          [ 0.0159,  0.3989,  1.0351,  ..., -0.5706, -0.1683, -0.8801],\n",
       "          [-0.0535,  0.7495,  0.8555,  ..., -0.2349, -0.2308, -1.0645]]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack all batch outputs, all_chunks WITHOUT `combined_output = torch.mean(all_chunks, dim=1)`  # example of mean pooling\n",
    "model = LongCodesEmbedder()\n",
    "# texts = np.ndarray([[\"Your very long text of 2000 tokens goes here...\", \"Another long text...\"]])\n",
    "# print(texts.shape)\n",
    "# embeddings = model(texts)\n",
    "\n",
    "text_output = model(codes_flatten)\n",
    "print(text_output.shape)\n",
    "text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([5, 512])\n",
      "torch.Size([3, 1, 2560, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3144, -0.0121,  0.0405,  ..., -0.5311,  0.5728, -0.1658],\n",
       "          [-0.5853,  0.2675, -0.1129,  ..., -0.8531,  0.3428,  0.0660],\n",
       "          [-0.6087,  0.2474, -0.0817,  ..., -0.8487,  0.2745,  0.0873],\n",
       "          ...,\n",
       "          [ 0.2932,  0.3740,  1.2786,  ..., -0.5581, -0.0460, -1.3185],\n",
       "          [ 0.0159,  0.3989,  1.0351,  ..., -0.5706, -0.1683, -0.8801],\n",
       "          [-0.0535,  0.7495,  0.8555,  ..., -0.2349, -0.2308, -1.0645]]],\n",
       "\n",
       "\n",
       "        [[[-0.3144, -0.0121,  0.0405,  ..., -0.5311,  0.5728, -0.1658],\n",
       "          [-0.5853,  0.2675, -0.1129,  ..., -0.8531,  0.3428,  0.0660],\n",
       "          [-0.6087,  0.2474, -0.0817,  ..., -0.8487,  0.2745,  0.0873],\n",
       "          ...,\n",
       "          [ 0.2932,  0.3740,  1.2786,  ..., -0.5581, -0.0460, -1.3185],\n",
       "          [ 0.0159,  0.3989,  1.0351,  ..., -0.5706, -0.1683, -0.8801],\n",
       "          [-0.0535,  0.7495,  0.8555,  ..., -0.2349, -0.2308, -1.0645]]],\n",
       "\n",
       "\n",
       "        [[[-0.3144, -0.0121,  0.0405,  ..., -0.5311,  0.5728, -0.1658],\n",
       "          [-0.5853,  0.2675, -0.1129,  ..., -0.8531,  0.3428,  0.0660],\n",
       "          [-0.6087,  0.2474, -0.0817,  ..., -0.8487,  0.2745,  0.0873],\n",
       "          ...,\n",
       "          [ 0.2932,  0.3740,  1.2786,  ..., -0.5581, -0.0460, -1.3185],\n",
       "          [ 0.0159,  0.3989,  1.0351,  ..., -0.5706, -0.1683, -0.8801],\n",
       "          [-0.0535,  0.7495,  0.8555,  ..., -0.2349, -0.2308, -1.0645]]]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_output = model(batch_codes_flatten)\n",
    "print(text_output.shape)\n",
    "text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2612\n"
     ]
    }
   ],
   "source": [
    "# The computation method for tokens dim \n",
    "if 2000 - (((2000-462) // 462 ) * 462 + 462) > 0 :\n",
    "    print(2000+((2000-462) // 462 )*50+462)\n",
    "else:\n",
    "    print(2000+((2000-462) // 462 )*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(2560-512)\n",
    "print((2560-512)//462)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2000//462 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
