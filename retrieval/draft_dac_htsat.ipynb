{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/home/dhuang/thesis/WavCaps\n",
      "/home/dhuang/.conda/envs/fs/lib/python310.zip\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/lib-dynload\n",
      "\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages/huggingface_hub-0.14.1-py3.8.egg\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg\n",
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages/sentencepiece-0.1.95-py3.10-linux-x86_64.egg\n",
      "/gpfs/home/dhuang/thesis/WavCaps/retrieval/models\n",
      "/gpfs/home/dhuang/thesis/WavCaps/retrieval/tools\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/39299838/how-do-i-import-module-in-jupyter-notebook-directory-into-notebooks-in-lower-dir\n",
    "# https://stackoverflow.com/questions/68572852/import-local-modules-in-jupyter-notebook\n",
    "import os\n",
    "import sys\n",
    "# os.path.split(os.getcwd())[0]\n",
    "sys.path.append(\"/gpfs/home/dhuang/thesis/WavCaps/retrieval/models\")\n",
    "sys.path.append(\"/gpfs/home/dhuang/thesis/WavCaps/retrieval/tools\")\n",
    "for nb_dir in sys.path:\n",
    "    print(nb_dir)\n",
    "    # sys.path.append(nb_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.audio_encoder import AudioEncoder\n",
    "from models.text_encoder import TextEncoder\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from tools.losses import AudioTextContrastiveLoss, NTXent\n",
    "from tools.utils import remove_grad\n",
    "import ruamel.yaml as yaml\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = './settings/dac_embedding.yaml'\n",
    "\n",
    "with open(config_file_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveform shape before crop:  (240000,)\n",
      "waveform shape:  (160000,)\n",
      "waveform_tensor shape:  torch.Size([1, 160000])\n"
     ]
    }
   ],
   "source": [
    "# Load audio signal file\n",
    "# wav_file_path = '../../dac/audio_samples/at2_32khz_cvt.wav'\n",
    "wav_file_path = '../../dac/audio_samples/at2_cvt.wav'\n",
    "waveform, _ = librosa.load(wav_file_path, sr=config[\"audio_args\"][\"sr\"], mono=True)\n",
    "print('waveform shape before crop: ', waveform.shape)\n",
    "if config[\"audio_args\"][\"max_length\"] != 0:\n",
    "            # if audio length is longer than max_length, we random crop it\n",
    "            max_length = config[\"audio_args\"][\"max_length\"] * config[\"audio_args\"][\"sr\"]\n",
    "            if waveform.shape[-1] > max_length:\n",
    "                max_start = waveform.shape[-1] - max_length\n",
    "                start = random.randint(0, max_start)\n",
    "                waveform = waveform[start: start + max_length]\n",
    "                \n",
    "print('waveform shape: ', waveform.shape)\n",
    "waveform_tensor = torch.tensor(waveform[None, :])\n",
    "print('waveform_tensor shape: ', waveform_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages/torchlibrosa/stft.py:686: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n"
     ]
    }
   ],
   "source": [
    "from models.feature_extractor import AudioFeature, DACLatents\n",
    "AF = AudioFeature(config[\"audio_args\"])\n",
    "DAC_Latents_Enc = DACLatents(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_encoded.shape:  torch.Size([1, 1, 1001, 64])\n"
     ]
    }
   ],
   "source": [
    "audio_encoded = AF(waveform_tensor)\n",
    "print('audio_encoded.shape: ', audio_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "audio_encoded.shape:  torch.Size([1, 64, 1001, 1])\n",
      "audio_encoded.shape:  torch.Size([1, 64, 1001, 1])\n",
      "audio_encoded.shape:  torch.Size([1, 1, 1001, 64])\n"
     ]
    }
   ],
   "source": [
    "bn0 = nn.BatchNorm2d(64)  #(self.config[\"audio_args\"][\"n_mels\"])\n",
    "# print('bn0.shape: ', bn0.shape)\n",
    "print(bn0)\n",
    "\n",
    "audio_encoded = audio_encoded.transpose(1, 3)\n",
    "print('audio_encoded.shape: ', audio_encoded.shape)\n",
    "# print(audio_encoded)\n",
    "\n",
    "audio_encoded = bn0(audio_encoded)\n",
    "print('audio_encoded.shape: ', audio_encoded.shape)\n",
    "# print(audio_encoded)\n",
    "\n",
    "audio_encoded = audio_encoded.transpose(1, 3)\n",
    "print('audio_encoded.shape: ', audio_encoded.shape)\n",
    "# print(audio_encoded)\n",
    "\n",
    "# x = self.reshape_wav2img(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 500])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lats = DAC_Latents_Enc(waveform_tensor)\n",
    "lats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 96, 500)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b,c = lats.shape\n",
    "a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"HTSAT based on the Swin Transformer\n",
    "Args:\n",
    "    spec_size (int | tuple(int)): Input Spectrogram size. Default 256\n",
    "    patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "    path_stride (iot | tuple(int)): Patch Stride for Frequency and Time Axis. Default: 4\n",
    "    in_chans (int): Number of input image channels. Default: 1 (mono)\n",
    "    num_classes (int): Number of classes for classification head. Default: 527\n",
    "    embed_dim (int): Patch embedding dimension. Default: 96\n",
    "    depths (tuple(int)): Depth of each HTSAT-Swin Transformer layer.\n",
    "    num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "    window_size (int): Window size. Default: 8\n",
    "    mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "    qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "    qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "    drop_rate (float): Dropout rate. Default: 0\n",
    "    attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "    drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "    norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "    ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "    patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "    use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    config (module): The configuration Module from config.py\n",
    "\"\"\"\n",
    "\n",
    "# self.freq_ratio = self.spec_size // self.config[\"audio_args\"][\"n_mels\"] = 256//64 = 4\n",
    "# target_T = int(self.spec_size * self.freq_ratio) = 256*4 = 1024\n",
    "# target_F = self.spec_size // self.freq_ratio = 256//4 = 64\n",
    "\n",
    "# [B, C, T, F] = [B, 1, <=1024, <= 64]  ==>>  1024*64 = 65536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256//(256//96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(7 - 7%3) // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_encoder = AudioEncoder(config)\n",
    "# settings for projection layers\n",
    "embed_size = config[\"embed_size\"]\n",
    "audio_width = audio_encoder.audio_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_encoded.shape:  torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # audio_encoded = audio_encoder.audio_enc(np.ndarray([[0], waveform]) )\n",
    "    audio_encoded = audio_encoder.audio_enc(waveform_tensor)\n",
    "    # audio_encoded = audio_encoder.audio_enc(torch.randn(1, 1, 64, 1024))\n",
    "    # audio_feats = audio_encoder(waveform)\n",
    "    # audio_embeds = F.normalize(audio_proj(audio_feats), dim=-1)\n",
    "\n",
    "print('audio_encoded.shape: ', audio_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key contains:  sed_model.spectrogram_extractor.stft.conv_real.weight\n",
      "key contains:  sed_model.spectrogram_extractor.stft.conv_imag.weight\n",
      "key contains:  sed_model.logmel_extractor.melW\n",
      "key contains:  sed_model.bn0.weight\n",
      "key contains:  sed_model.bn0.bias\n",
      "key contains:  sed_model.bn0.running_mean\n",
      "key contains:  sed_model.bn0.running_var\n",
      "key contains:  sed_model.bn0.num_batches_tracked\n",
      "key contains:  sed_model.patch_embed.proj.weight\n",
      "key contains:  sed_model.patch_embed.proj.bias\n",
      "key contains:  sed_model.patch_embed.norm.weight\n",
      "key contains:  sed_model.patch_embed.norm.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.norm1.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.norm1.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.relative_position_index\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.qkv.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.qkv.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.proj.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.proj.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.norm2.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.norm2.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.attn_mask\n",
      "key contains:  sed_model.layers.0.blocks.1.norm1.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.norm1.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.relative_position_index\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.qkv.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.qkv.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.proj.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.proj.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.norm2.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.norm2.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.0.downsample.reduction.weight\n",
      "key contains:  sed_model.layers.0.downsample.norm.weight\n",
      "key contains:  sed_model.layers.0.downsample.norm.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.norm1.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.norm1.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.relative_position_index\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.qkv.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.qkv.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.proj.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.proj.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.norm2.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.norm2.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.attn_mask\n",
      "key contains:  sed_model.layers.1.blocks.1.norm1.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.norm1.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.relative_position_index\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.qkv.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.qkv.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.proj.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.proj.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.norm2.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.norm2.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.1.downsample.reduction.weight\n",
      "key contains:  sed_model.layers.1.downsample.norm.weight\n",
      "key contains:  sed_model.layers.1.downsample.norm.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.attn_mask\n",
      "key contains:  sed_model.layers.2.blocks.1.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.attn_mask\n",
      "key contains:  sed_model.layers.2.blocks.3.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.attn_mask\n",
      "key contains:  sed_model.layers.2.blocks.5.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.downsample.reduction.weight\n",
      "key contains:  sed_model.layers.2.downsample.norm.weight\n",
      "key contains:  sed_model.layers.2.downsample.norm.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.norm1.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.norm1.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.relative_position_index\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.qkv.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.qkv.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.proj.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.proj.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.norm2.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.norm2.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.norm1.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.norm1.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.relative_position_index\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.qkv.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.qkv.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.proj.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.proj.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.norm2.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.norm2.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.mlp.fc2.bias\n",
      "key contains:  sed_model.norm.weight\n",
      "key contains:  sed_model.norm.bias\n",
      "key contains:  sed_model.tscam_conv.weight\n",
      "key contains:  sed_model.tscam_conv.bias\n",
      "key contains:  sed_model.head.weight\n",
      "key contains:  sed_model.head.bias\n",
      "audio_feats_extractor.mel_trans.stft.conv_real.weight \t Unloaded\n",
      "audio_feats_extractor.mel_trans.stft.conv_imag.weight \t Unloaded\n",
      "audio_feats_extractor.log_trans.melW \t Unloaded\n",
      "bn0.weight \t Loaded\n",
      "bn0.bias \t Loaded\n",
      "patch_embed.proj.weight \t Loaded\n",
      "patch_embed.proj.bias \t Loaded\n",
      "patch_embed.norm.weight \t Loaded\n",
      "patch_embed.norm.bias \t Loaded\n",
      "layers.0.blocks.0.norm1.weight \t Loaded\n",
      "layers.0.blocks.0.norm1.bias \t Loaded\n",
      "layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "layers.0.blocks.0.norm2.weight \t Loaded\n",
      "layers.0.blocks.0.norm2.bias \t Loaded\n",
      "layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "layers.0.blocks.1.norm1.weight \t Loaded\n",
      "layers.0.blocks.1.norm1.bias \t Loaded\n",
      "layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "layers.0.blocks.1.norm2.weight \t Loaded\n",
      "layers.0.blocks.1.norm2.bias \t Loaded\n",
      "layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "layers.0.downsample.reduction.weight \t Loaded\n",
      "layers.0.downsample.norm.weight \t Loaded\n",
      "layers.0.downsample.norm.bias \t Loaded\n",
      "layers.1.blocks.0.norm1.weight \t Loaded\n",
      "layers.1.blocks.0.norm1.bias \t Loaded\n",
      "layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "layers.1.blocks.0.norm2.weight \t Loaded\n",
      "layers.1.blocks.0.norm2.bias \t Loaded\n",
      "layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "layers.1.blocks.1.norm1.weight \t Loaded\n",
      "layers.1.blocks.1.norm1.bias \t Loaded\n",
      "layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "layers.1.blocks.1.norm2.weight \t Loaded\n",
      "layers.1.blocks.1.norm2.bias \t Loaded\n",
      "layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "layers.1.downsample.reduction.weight \t Loaded\n",
      "layers.1.downsample.norm.weight \t Loaded\n",
      "layers.1.downsample.norm.bias \t Loaded\n",
      "layers.2.blocks.0.norm1.weight \t Loaded\n",
      "layers.2.blocks.0.norm1.bias \t Loaded\n",
      "layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.0.norm2.weight \t Loaded\n",
      "layers.2.blocks.0.norm2.bias \t Loaded\n",
      "layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.1.norm1.weight \t Loaded\n",
      "layers.2.blocks.1.norm1.bias \t Loaded\n",
      "layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.1.norm2.weight \t Loaded\n",
      "layers.2.blocks.1.norm2.bias \t Loaded\n",
      "layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.2.norm1.weight \t Loaded\n",
      "layers.2.blocks.2.norm1.bias \t Loaded\n",
      "layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.2.norm2.weight \t Loaded\n",
      "layers.2.blocks.2.norm2.bias \t Loaded\n",
      "layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.3.norm1.weight \t Loaded\n",
      "layers.2.blocks.3.norm1.bias \t Loaded\n",
      "layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.3.norm2.weight \t Loaded\n",
      "layers.2.blocks.3.norm2.bias \t Loaded\n",
      "layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.4.norm1.weight \t Loaded\n",
      "layers.2.blocks.4.norm1.bias \t Loaded\n",
      "layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.4.norm2.weight \t Loaded\n",
      "layers.2.blocks.4.norm2.bias \t Loaded\n",
      "layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.5.norm1.weight \t Loaded\n",
      "layers.2.blocks.5.norm1.bias \t Loaded\n",
      "layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.5.norm2.weight \t Loaded\n",
      "layers.2.blocks.5.norm2.bias \t Loaded\n",
      "layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "layers.2.downsample.reduction.weight \t Loaded\n",
      "layers.2.downsample.norm.weight \t Loaded\n",
      "layers.2.downsample.norm.bias \t Loaded\n",
      "layers.3.blocks.0.norm1.weight \t Loaded\n",
      "layers.3.blocks.0.norm1.bias \t Loaded\n",
      "layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "layers.3.blocks.0.norm2.weight \t Loaded\n",
      "layers.3.blocks.0.norm2.bias \t Loaded\n",
      "layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "layers.3.blocks.1.norm1.weight \t Loaded\n",
      "layers.3.blocks.1.norm1.bias \t Loaded\n",
      "layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "layers.3.blocks.1.norm2.weight \t Loaded\n",
      "layers.3.blocks.1.norm2.bias \t Loaded\n",
      "layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "norm.weight \t Loaded\n",
      "norm.bias \t Loaded\n",
      "tscam_conv.weight \t Loaded\n",
      "tscam_conv.bias \t Loaded\n",
      "head.weight \t Loaded\n",
      "head.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "from models.htsat import HTSAT_Swin_Transformer\n",
    "\n",
    "htsat_audio_enc = HTSAT_Swin_Transformer(\n",
    "                spec_size=256,\n",
    "                patch_size=4,\n",
    "                patch_stride=(4, 4),\n",
    "                num_classes=527,\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 6, 2],\n",
    "                num_heads=[4, 8, 16, 32],\n",
    "                window_size=8,\n",
    "                config=config,\n",
    "            )\n",
    "audio_ckpt = torch.load(\"../../pretrained_models/audio_encoder/HTSAT.ckpt\", map_location=\"cpu\")[\"state_dict\"]\n",
    "for key in list(audio_ckpt.keys()):\n",
    "    print(\"key contains: \", key)\n",
    "    if key.startswith('sed_model') and ('spectrogram_extractor' not in key\n",
    "                                        and 'logmel_extractor' not in key):\n",
    "        v = audio_ckpt.pop(key)\n",
    "        audio_ckpt[key[10:]] = v\n",
    "htsat_audio_enc.load_state_dict(audio_ckpt, strict=False)\n",
    "param_names = [n for n, p in htsat_audio_enc.named_parameters()]\n",
    "for n in param_names:\n",
    "    print(n, \"\\t\", \"Loaded\" if n in audio_ckpt else \"Unloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhuang/.conda/envs/fs/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995026/work/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key contains:  sed_model.spectrogram_extractor.stft.conv_real.weight\n",
      "key contains:  sed_model.spectrogram_extractor.stft.conv_imag.weight\n",
      "key contains:  sed_model.logmel_extractor.melW\n",
      "key contains:  sed_model.bn0.weight\n",
      "key contains:  sed_model.bn0.bias\n",
      "key contains:  sed_model.bn0.running_mean\n",
      "key contains:  sed_model.bn0.running_var\n",
      "key contains:  sed_model.bn0.num_batches_tracked\n",
      "key contains:  sed_model.patch_embed.proj.weight\n",
      "key contains:  sed_model.patch_embed.proj.bias\n",
      "key contains:  sed_model.patch_embed.norm.weight\n",
      "key contains:  sed_model.patch_embed.norm.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.norm1.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.norm1.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.relative_position_index\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.qkv.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.qkv.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.proj.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.attn.proj.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.norm2.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.norm2.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.0.blocks.0.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.0.blocks.0.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.attn_mask\n",
      "key contains:  sed_model.layers.0.blocks.1.norm1.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.norm1.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.relative_position_index\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.qkv.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.qkv.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.proj.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.attn.proj.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.norm2.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.norm2.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.0.blocks.1.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.0.blocks.1.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.0.downsample.reduction.weight\n",
      "key contains:  sed_model.layers.0.downsample.norm.weight\n",
      "key contains:  sed_model.layers.0.downsample.norm.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.norm1.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.norm1.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.relative_position_index\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.qkv.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.qkv.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.proj.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.attn.proj.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.norm2.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.norm2.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.1.blocks.0.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.1.blocks.0.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.attn_mask\n",
      "key contains:  sed_model.layers.1.blocks.1.norm1.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.norm1.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.relative_position_index\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.qkv.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.qkv.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.proj.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.attn.proj.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.norm2.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.norm2.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.1.blocks.1.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.1.blocks.1.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.1.downsample.reduction.weight\n",
      "key contains:  sed_model.layers.1.downsample.norm.weight\n",
      "key contains:  sed_model.layers.1.downsample.norm.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.0.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.0.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.attn_mask\n",
      "key contains:  sed_model.layers.2.blocks.1.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.1.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.1.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.2.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.2.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.attn_mask\n",
      "key contains:  sed_model.layers.2.blocks.3.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.3.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.3.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.4.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.4.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.attn_mask\n",
      "key contains:  sed_model.layers.2.blocks.5.norm1.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.norm1.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.relative_position_index\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.qkv.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.qkv.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.proj.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.attn.proj.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.norm2.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.norm2.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.2.blocks.5.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.2.blocks.5.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.2.downsample.reduction.weight\n",
      "key contains:  sed_model.layers.2.downsample.norm.weight\n",
      "key contains:  sed_model.layers.2.downsample.norm.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.norm1.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.norm1.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.relative_position_index\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.qkv.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.qkv.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.proj.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.attn.proj.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.norm2.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.norm2.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.3.blocks.0.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.3.blocks.0.mlp.fc2.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.norm1.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.norm1.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.relative_position_bias_table\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.relative_position_index\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.qkv.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.qkv.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.proj.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.attn.proj.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.norm2.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.norm2.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.mlp.fc1.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.mlp.fc1.bias\n",
      "key contains:  sed_model.layers.3.blocks.1.mlp.fc2.weight\n",
      "key contains:  sed_model.layers.3.blocks.1.mlp.fc2.bias\n",
      "key contains:  sed_model.norm.weight\n",
      "key contains:  sed_model.norm.bias\n",
      "key contains:  sed_model.tscam_conv.weight\n",
      "key contains:  sed_model.tscam_conv.bias\n",
      "key contains:  sed_model.head.weight\n",
      "key contains:  sed_model.head.bias\n",
      "bn0.weight \t Loaded\n",
      "bn0.bias \t Loaded\n",
      "patch_embed.proj.weight \t Loaded\n",
      "patch_embed.proj.bias \t Loaded\n",
      "patch_embed.norm.weight \t Loaded\n",
      "patch_embed.norm.bias \t Loaded\n",
      "layers.0.blocks.0.norm1.weight \t Loaded\n",
      "layers.0.blocks.0.norm1.bias \t Loaded\n",
      "layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "layers.0.blocks.0.norm2.weight \t Loaded\n",
      "layers.0.blocks.0.norm2.bias \t Loaded\n",
      "layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "layers.0.blocks.1.norm1.weight \t Loaded\n",
      "layers.0.blocks.1.norm1.bias \t Loaded\n",
      "layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "layers.0.blocks.1.norm2.weight \t Loaded\n",
      "layers.0.blocks.1.norm2.bias \t Loaded\n",
      "layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "layers.0.downsample.reduction.weight \t Loaded\n",
      "layers.0.downsample.norm.weight \t Loaded\n",
      "layers.0.downsample.norm.bias \t Loaded\n",
      "layers.1.blocks.0.norm1.weight \t Loaded\n",
      "layers.1.blocks.0.norm1.bias \t Loaded\n",
      "layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "layers.1.blocks.0.norm2.weight \t Loaded\n",
      "layers.1.blocks.0.norm2.bias \t Loaded\n",
      "layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "layers.1.blocks.1.norm1.weight \t Loaded\n",
      "layers.1.blocks.1.norm1.bias \t Loaded\n",
      "layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "layers.1.blocks.1.norm2.weight \t Loaded\n",
      "layers.1.blocks.1.norm2.bias \t Loaded\n",
      "layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "layers.1.downsample.reduction.weight \t Loaded\n",
      "layers.1.downsample.norm.weight \t Loaded\n",
      "layers.1.downsample.norm.bias \t Loaded\n",
      "layers.2.blocks.0.norm1.weight \t Loaded\n",
      "layers.2.blocks.0.norm1.bias \t Loaded\n",
      "layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.0.norm2.weight \t Loaded\n",
      "layers.2.blocks.0.norm2.bias \t Loaded\n",
      "layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.1.norm1.weight \t Loaded\n",
      "layers.2.blocks.1.norm1.bias \t Loaded\n",
      "layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.1.norm2.weight \t Loaded\n",
      "layers.2.blocks.1.norm2.bias \t Loaded\n",
      "layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.2.norm1.weight \t Loaded\n",
      "layers.2.blocks.2.norm1.bias \t Loaded\n",
      "layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.2.norm2.weight \t Loaded\n",
      "layers.2.blocks.2.norm2.bias \t Loaded\n",
      "layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.3.norm1.weight \t Loaded\n",
      "layers.2.blocks.3.norm1.bias \t Loaded\n",
      "layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.3.norm2.weight \t Loaded\n",
      "layers.2.blocks.3.norm2.bias \t Loaded\n",
      "layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.4.norm1.weight \t Loaded\n",
      "layers.2.blocks.4.norm1.bias \t Loaded\n",
      "layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.4.norm2.weight \t Loaded\n",
      "layers.2.blocks.4.norm2.bias \t Loaded\n",
      "layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "layers.2.blocks.5.norm1.weight \t Loaded\n",
      "layers.2.blocks.5.norm1.bias \t Loaded\n",
      "layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "layers.2.blocks.5.norm2.weight \t Loaded\n",
      "layers.2.blocks.5.norm2.bias \t Loaded\n",
      "layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "layers.2.downsample.reduction.weight \t Loaded\n",
      "layers.2.downsample.norm.weight \t Loaded\n",
      "layers.2.downsample.norm.bias \t Loaded\n",
      "layers.3.blocks.0.norm1.weight \t Loaded\n",
      "layers.3.blocks.0.norm1.bias \t Loaded\n",
      "layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "layers.3.blocks.0.norm2.weight \t Loaded\n",
      "layers.3.blocks.0.norm2.bias \t Loaded\n",
      "layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "layers.3.blocks.1.norm1.weight \t Loaded\n",
      "layers.3.blocks.1.norm1.bias \t Loaded\n",
      "layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "layers.3.blocks.1.norm2.weight \t Loaded\n",
      "layers.3.blocks.1.norm2.bias \t Loaded\n",
      "layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "norm.weight \t Loaded\n",
      "norm.bias \t Loaded\n",
      "tscam_conv.weight \t Loaded\n",
      "tscam_conv.bias \t Loaded\n",
      "head.weight \t Loaded\n",
      "head.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "from models.Codes_htsat import Codec_Swin_Transformer\n",
    "\n",
    "htsat_audio_enc = Codec_Swin_Transformer(\n",
    "                spec_size=256,\n",
    "                patch_size=4,\n",
    "                patch_stride=(4, 4),\n",
    "                num_classes=527,\n",
    "                embed_dim=96,\n",
    "                depths=[2, 2, 6, 2],\n",
    "                num_heads=[4, 8, 16, 32],\n",
    "                window_size=8,\n",
    "                config=config,\n",
    "            )\n",
    "audio_ckpt = torch.load(\"../../pretrained_models/audio_encoder/HTSAT.ckpt\", map_location=\"cpu\")[\"state_dict\"]\n",
    "for key in list(audio_ckpt.keys()):\n",
    "    print(\"key contains: \", key)\n",
    "    if key.startswith('sed_model') and ('spectrogram_extractor' not in key\n",
    "                                        and 'logmel_extractor' not in key):\n",
    "        v = audio_ckpt.pop(key)\n",
    "        audio_ckpt[key[10:]] = v\n",
    "htsat_audio_enc.load_state_dict(audio_ckpt, strict=False)\n",
    "param_names = [n for n, p in htsat_audio_enc.named_parameters()]\n",
    "for n in param_names:\n",
    "    print(n, \"\\t\", \"Loaded\" if n in audio_ckpt else \"Unloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
